# SiteMainer

Un potente strumento di analisi web che consente di scansionare una lista di URL e valutare la rilevanza dei contenuti attraverso ricerca di parole chiave e analisi semantica basata su AI.

## ğŸš€ Caratteristiche

- **Analisi multi-URL**: Scansiona automaticamente una lista di siti web
- **Doppio scoring system**:
  - **Keyword Score**: Combina diversitÃ  e frequenza delle parole chiave trovate
  - **Semantic Score**: Utilizza embeddings AI per valutare la correlazione semantica
- **Scansione flessibile**: Opzione per analizzare solo la homepage o esplorare il sito in profonditÃ 
- **Export Excel**: Esporta i risultati in formato .xlsx per ulteriori analisi
- **Interface intuitiva**: Web app costruita con Streamlit

## ğŸ“‹ Prerequisiti

- Python 3.8+
- Dipendenze elencate in `requirements.txt`

## ğŸ› ï¸ Installazione

1. Clona il repository:
```bash
git clone https://github.com/[username]/sitemainer.git
cd sitemainer
```

2. Installa le dipendenze:
```bash
pip install -r requirements.txt
```

3. Avvia l'applicazione:
```bash
streamlit run main.py
```

## ğŸ“ Struttura del progetto

```
sitemainer/
â”‚
â”œâ”€â”€ main.py              # Script principale dell'applicazione
â”œâ”€â”€ utils.py             # Funzioni di utilitÃ  per web scraping e analisi
â”œâ”€â”€ requirements.txt     # Dipendenze Python
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Template.xlsx    # Template di esempio per l'input
â””â”€â”€ img/
    â”œâ”€â”€ logo.png         # Logo dell'applicazione
    â””â”€â”€ favicon.png      # Favicon
```

## ğŸ¯ Come utilizzare

### 1. Preparazione del file di input

Crea un file Excel (.xlsx) con due fogli:

- **websites**: Contiene la colonna `url` con la lista dei siti da analizzare
- **keywords**: Contiene la colonna `keywords` con le parole chiave da ricercare

Puoi scaricare il template di esempio direttamente dall'applicazione.

### 2. Configurazione dei parametri

L'applicazione offre diverse opzioni di configurazione:

#### Parametri di scansione:
- **Deep scan**: Abilita la scansione di piÃ¹ pagine per sito
- **Numero massimo pagine**: Limita le pagine da scansionare per sito (1-50)
- **ProfonditÃ  massima**: ProfonditÃ  nell'alberatura del sito (1-5)
- **Delay**: Pausa tra le richieste per evitare rate limiting (0-5 secondi)

#### Pesi del keyword score:
- **DiversitÃ **: Peso per la varietÃ  delle keyword trovate
- **Frequenza**: Peso per la frequenza delle keyword

#### Pesi del final score:
- **SimilaritÃ  semantica**: Peso dell'analisi AI semantica
- **Presenza keyword**: Peso della ricerca esatta delle keyword

### 3. Esecuzione dell'analisi

1. Carica il file Excel preparato
2. Configura i parametri desiderati
3. Clicca su "â–¶ï¸ Avvia l'analisi"
4. Attendi il completamento (visualizzazione del progresso in tempo reale)
5. Scarica i risultati in formato Excel

## ğŸ“Š Output e metriche

L'analisi produce diverse metriche per ogni URL:

- **keyword_score** (0-1): Combina diversitÃ  e frequenza delle keyword
- **semantic_score** (0-1): Misura la correlazione semantica tramite AI
- **final_score** (0-1): Punteggio finale pesato
- **text**: Contenuto testuale estratto
- **pages_scanned**: Numero di pagine analizzate
- **keyword_matches**: Lista delle keyword trovate

## âš¡ Prestazioni e limitazioni

- **Testato con**: Fino a 5.000 URL (solo homepage)
- **Raccomandazioni**: 
  - Per liste grandi, suddividere in piÃ¹ file
  - Utilizzare delay appropriati per evitare rate limiting
  - Monitorare la connessione di rete durante l'elaborazione

## ğŸ§  Tecnologie utilizzate

- **Streamlit**: Framework per l'interfaccia web
- **Sentence Transformers**: Modello AI per embeddings semantici
- **BeautifulSoup**: Web scraping e parsing HTML
- **Pandas**: Manipolazione dati

## ğŸ“ Licenza

Distribuito sotto licenza MIT. Vedi `LICENSE` per maggiori informazioni.

## ğŸ“ Supporto

Per domande, bug report o richieste di funzionalitÃ , apri un issue su GitHub.

---

**Note**: Questo strumento Ã¨ progettato per analisi di contenuti pubblicamente accessibili. Assicurati di rispettare i termini di servizio dei siti web analizzati e le policy sui robots.txt.